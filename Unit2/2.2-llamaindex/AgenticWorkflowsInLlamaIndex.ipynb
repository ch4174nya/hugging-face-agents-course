{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agentic Workflows in LlamaIndex\n",
    "\n",
    "A workflow in LlamaIndex provides a structured way to organize your code into sequential and manageable steps.\n",
    "\n",
    "`Workflow`s strike a balance between the autonomy of agents while maintaining control of the overall workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflow creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step my_step\n",
      "Step my_step produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello World!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent)-> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result='Hello World!')\n",
    "    \n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=True)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting multiple steps\n",
    "Connect multiple steps through custom events that carry data between steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step step_one\n",
      "Step step_one produced event ProcessingEvent\n",
      "Running step step_two\n",
      "Step step_two produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished Processing: Step 1 processed'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent)-> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result='Step 1 processed')\n",
    "    \n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent)-> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f'Finished Processing: {ev.intermediate_result}'\n",
    "        return StopEvent(result=final_result)\n",
    "    \n",
    "w = MultiStepWorkflow(timeout=10, verbose=True)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loops and Branches\n",
    "\n",
    "`Type hinting` is the most powerful part of workflows because it allows us to create branches, loops and joins to facilitate more complex workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step step_one\n",
      "Good thing happened\n",
      "Step step_one produced event ProcessingEvent\n",
      "Running step step_two\n",
      "Step step_two produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tabnanny import verbose\n",
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print('Bad thing happened')\n",
    "            return LoopEvent(loop_output='Back to step 1')\n",
    "\n",
    "        else:\n",
    "            print('Good thing happened')\n",
    "            return ProcessingEvent(intermediate_result='First step complete')\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f'Finished processing: {ev.intermediate_result}'\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=True)\n",
    "result = await w.run()\n",
    "result\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drawing Workflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gtk-Message: 21:06:26.865: Not loading module \"atk-bridge\": The functionality is provided by GTK natively. Please try to not load it.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "draw_all_possible_flows(w, 'flow.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Management\n",
    "\n",
    "This uses the `Context` object we saw in the `AgentsInLlamaIndex` Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running step step_one\n",
      "Step step_one produced event ProcessingEvent\n",
      "Running step step_two\n",
      "Query: What is the capital of France?\n",
      "Step step_two produced event StopEvent\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished Processing: Step 1 complete'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context, StartEvent, StopEvent, Event\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # process initial data:\n",
    "        await ctx.set('query', 'What is the capital of France?')\n",
    "        return ProcessingEvent(intermediate_result='Step 1 complete')\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.get('query')\n",
    "        print(f'Query: {query}')\n",
    "        final_result = f'Finished Processing: {ev.intermediate_result}'\n",
    "        return StopEvent(result=final_result)\n",
    "    \n",
    "w = MultiStepWorkflow(timeout=10, verbose=True)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Workflows\n",
    "\n",
    "Instead of manual `workflow` creation, we can use the `AgentWorkflow` class to create a multi-agent workflow. \n",
    "The `AgentWorkflow` uses `Workflow` Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities, allowing for complex agent systems where different agents handle different aspects of a task. \n",
    "One agent must be designated as the root agent in the `AgentWorkflow` constructor. When a user message comes in, it is first routed to the root agent.\n",
    "\n",
    "Each agent can then:\n",
    "- Handle the request directly using their tools\n",
    "- Handoff to another agent better suited for the task\n",
    "- Return a response to the user\n",
    "\n",
    "\n",
    "In the below system, we also add `Context` to show context sharing (as a simple usecase of keeping track of the number of function calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The sum of 5 and 3 is 8.\n",
      "State: 1\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.core.workflow import Context\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "async def add(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    # update count\n",
    "    cur_state = await ctx.get('state')\n",
    "    cur_state['num_fn_calls'] += 1\n",
    "    await ctx.set('state', cur_state)\n",
    "    \n",
    "    return a + b\n",
    "\n",
    "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "\n",
    "    # update count\n",
    "    cur_state = await ctx.get('state')\n",
    "    cur_state['num_fn_calls'] += 1\n",
    "    await ctx.set('state', cur_state)\n",
    "    \n",
    "    return a * b\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "llm = OpenAI('gpt-4o-mini')\n",
    "\n",
    "multiply_agent = ReActAgent(\n",
    "    name='multiply_agent',\n",
    "    description='Is able to multiple two numbers',\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools = [multiply],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name='addition_agent',\n",
    "    description='Is able to add two integers',\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools = [add],\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent='multiply_agent',\n",
    "    initial_state={'num_fn_calls': 0},\n",
    "    state_prompt='Current state: {state}. User Message: {msg}'\n",
    ")\n",
    "\n",
    "# Run the system, with context\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg = 'Can you add 5 and 3?', ctx=ctx)\n",
    "print(f'Response: {response}')\n",
    "\n",
    "# pull out and inspect state\n",
    "state = await ctx.get('state')\n",
    "print(f'State: {state[\"num_fn_calls\"]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
